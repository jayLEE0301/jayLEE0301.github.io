<meta charset="utf-8" emacsmode="-*- markdown -*-">

**SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning**

<p><center><a href="https://github.com/dsshim0125">Dongseok Shim*</a>, &emsp; <a href="https://sjlee.cc/">Seungjae Lee*</a>, &emsp; <a href="https://larr.snu.ac.kr/">H. Jin Kim</a></center></p>

<p><center> *Equal contribution; Order was determined by coin flip </center></p>

<p><center><b><a href="https://arxiv.org/abs/2301.11520">Paper</a>, &emsp; <a href="" onclick="alert('will be released after the decision')">Code</a></b></center></p>


![](images/snerl_thumbnail.jpg width="70%")


*__tldr__*:
As previous representations for reinforcement learning cannot effectively incorporate a human-intuitive understanding of the 3D environment, they usually suffer from sub-optimal performances. In this paper, we present Semantic-aware Neural Radiance Fields for Reinforcement Learning (SNeRL), which jointly optimizes semantic-aware neural radiance fields (NeRF) with a convolutional encoder to learn 3D-aware neural implicit representation from multi-view images. We introduce 3D semantic and distilled feature fields in parallel to the RGB radiance fields in NeRF to learn semantic and object-centric representation for reinforcement learning. SNeRL outperforms not only previous pixel-based representations but also recent 3D-aware representations both in model-free and model-based reinforcement learning.


![NeRL consists of two stages, which are pre-training NeRF-based autoencoder and fine-tuning to the downstream RL tasks, respectively. With observations from three different camera views, an encoder produces a single latent vector z, and a decoder with neural rendering function fÎ¸ takes the position x, viewing direction d in the 3D coordinates and z as inputs to synthesize three different fields in the arbitrary views. An auxiliary multi-view self-prediction loss is applied to enable view-invariant representation. Then, the encoder and the decoder are jointly optimized in a supervised manner with an offline dataset. The pre-trained encoder is utilized as a feature extractor to train the policy with off-the-shelf RL algorithms.](images/snerl_main.jpg)


![Episode returns of the evaluation results. Shading indicates a standard deviation across 4 seeds. The curves are not visible in the  Hammer-v2 environment as they overlap each other. Note that SNeRL in this figure is obtained without auxiliary loss (multi-view self-predictive presentation), which could enable further improvements in some environments.](images/snerl_main_result.jpg width="100%")

![Qualitative results on the image reconstruction in 3 different camera views via neural rendering.
The synthesized images from SNeRL achieve better fidelity compared to NeRF-RL in several environments.](images/snerl_recon.jpg width="100%")

--------------------------

<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js" charset="utf-8"></script><script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js" charset="utf-8"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
