<meta charset="utf-8" emacsmode="-*- markdown -*-">

**Outcome-directed Reinforcement Learning by Uncertainty & Temporal Distance-Aware Curriculum Goal Generation**

<p><center> ICLR 2023, Spotlight</center></p>

<p><center><a href="https://dscho1234.github.io">Daesol Cho*</a>, &emsp; <a href="https://sjlee.cc/">Seungjae Lee*</a>, &emsp; <a href="https://larr.snu.ac.kr/">H. Jin Kim</a></center></p>

<p><center> *Equal contribution </center></p>

<p><center><b><a href="https://arxiv.org/abs/2301.11741">Paper</a>, &emsp; <a href="https://github.com/jayLEE0301/outpace_official">Code</a></b></center></p>

*__tldr__*:
Current reinforcement learning (RL) often suffers when solving a challenging exploration problem where the desired outcomes or high rewards are rarely observed. Even though curriculum RL, a framework that solves complex tasks by proposing a sequence of surrogate tasks, shows reasonable results, most of the previous works still have difficulty in proposing curriculum due to the absence of a mechanism for obtaining calibrated guidance to the desired outcome state without any prior domain knowledge. To alleviate it, we propose an uncertainty \& temporal distance-aware curriculum goal generation method for the outcome-directed RL via solving a bipartite matching problem. It could not only provide precisely calibrated guidance of the curriculum to the desired outcome states but also bring much better sample efficiency and geometry-agnostic curriculum goal proposal capability compared to previous curriculum RL methods. We demonstrate that our algorithm significantly outperforms these prior methods in a variety of challenging navigation tasks and robotic manipulation tasks in a quantitative and qualitative way.


![OUTPACE proposes uncertainty and temporal distance-aware curriculum goals to enable the agent to progress toward the desired outcome state automatically. Note that the temporal distance estimation is reliable within the explored region where we query the curriculum goals.](images/thumbnail6.jpg)


![Average distance from the curriculum goals to the final goals (Lower is better). Our method's increasing tendencies at initial steps in some environments are due to the geometric structure of the environments themselves.](images/outpace_main_result.jpg width="100%")

![Visualization of the proposed curriculum goals. First row: Ant Locomotion, Second row: Point-N-Maze.](images/outpace_proposed_goals.jpg width="100%")


![Visualization of the uncertainty quantification along training progress (left) and trained $f^\pi_\phi(s)$ (right) in the Point-N-Maze environment. In the (right) figure, high reward means temporally close to the desired outcome states, and low reward means the opposite.](images/outpace_uncertainty_quantification.jpg width="100%")

--------------------------

<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js" charset="utf-8"></script><script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js" charset="utf-8"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
