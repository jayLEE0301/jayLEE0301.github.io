<!DOCTYPE HTML>
<html lang="en">
  <head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MBNHLWD0TG"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-MBNHLWD0TG');
    </script>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Seungjae (Jay) Lee</title>

    <meta name="author" content="Seungjae (Jay) Lee">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <div class="container">
      <!-- Header Section -->
      <header class="header">
        <div class="header-content">
          <div class="header-text">
            <h1 class="name">Seungjae (Jay) Lee</h1>
            <p>
              I am first-year Ph.D. student at UMD Department of Computer Science üíª, co-advised by professors
              <a href="https://furong-huang.com/">Furong Huang</a> and
              <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>.
            </p>
            <p>
              Prior to UMD, I had my Masters Degree in
              <a href="https://aerospace.snu.ac.kr/en">Department of Aerospace Engineering ‚úàÔ∏è at SNU</a>
              advised by <a href="https://larr.snu.ac.kr">Prof. H. Jin Kim</a>.
              I also spent time at Generalizable Robotics and AI Lab (GRAIL) ü§ñ at NYU, advised by
              <a href="https://www.lerrelpinto.com/">Prof. Lerrel Pinto</a>.
              I worked on enhancing the data efficiency of Reinforcement Learning (RL) and Imitation Learning (IL)
              systems and applied them to various decision-making scenarios, including real-world robots.
            </p>
            <p>
              Before that, I received Bachelor's degrees in
              <a href="https://me.snu.ac.kr/en/">Mechanical and Aerospace Engineering at SNU ‚öôÔ∏è.</a>
            </p>
            <p>
              "üíª + ‚úàÔ∏è + ü§ñ + ‚öôÔ∏è = Me"
            </p>
            <div class="social-links">
              <a href="mailto:sjaelee@umd.edu">Email</a> &nbsp;/&nbsp;
              <a href="data/sjlee_CV_final.pdf">CV</a> &nbsp;/&nbsp;
              <a href="https://scholar.google.com/citations?user=hpR9h74AAAAJ">Google Scholar</a> &nbsp;/&nbsp;
              <a href="https://twitter.com/JayLEE_0301">Twitter</a> &nbsp;/&nbsp;
              <a href="https://github.com/jayLEE0301">Github</a>
            </div>
          </div>
          <div class="header-image">
            <a href="images/sjlee.jpg">
              <img src="images/sjlee.jpg" alt="profile photo" class="profile-photo">
            </a>
          </div>
        </div>
      </header>

      <!-- News Section -->
      <!-- <section class="section">
        <h2>News</h2>
        <ul class="news-list">
          <li>05/2024: A paper on multi-modal behavior generation for robot agents were accepted to
            <a href="https://icml.cc/Conferences/2024/">ICML 2024</a> (Spotlight).</li>
          <li>02/2024: Graduated from the master's program at Seoul National University (Aerospace Engineering).</li>
          <li>12/2023: Presented two papers on curriculum learning for robot agents at
            <a href="https://nips.cc/Conferences/2023">NeurIPS 2023.</a></li>
          <li>07/2023: Presented a paper on 3D representation learning for robot agents at
            <a href="https://icml.cc/Conferences/2022">ICML 2023.</a></li>
          <li>05/2023: Presented a paper on exploration for RL at
            <a href="https://iclr.cc/Conferences/2023">ICLR 2023</a> (Spotlight).</li>
        </ul>
      </section> -->

      <!-- Education Section -->
      <section class="section">
        <h2>Education & Affiliations</h2>
        <div class="education-list">
          <div class="education-item">
            <img src='images/umd_logo.webp' alt="UMD Logo" class="institution-logo">
            <div class="education-details">
              <h3>Ph.D. in Computer Science</h3>
              <p>Advised by Professor Furong Huang and Professor Jia-Bin Huang.</p>
              <p><em>Aug 2024 - Present | College Park, MD</em></p>
            </div>
          </div>
          <div class="education-item">
            <img src='images/nyu_logo.png' alt="NYU Logo" class="institution-logo">
            <div class="education-details">
              <h3>Visiting Research</h3>
              <p>Advised by Professor Lerrel Pinto.</p>
              <p><em>Jul 2023 - Jun 2024 | New York, NY</em></p>
            </div>
          </div>
          <div class="education-item">
            <img src='images/snu_logo.png' alt="SNU Logo" class="institution-logo">
            <div class="education-details">
              <h3>M.S. in Aerospace Engineering</h3>
              <p>Advised by Professor H. Jin Kim.</p>
              <p><em>Mar 2021 - Feb 2024 | Seoul, Korea</em></p>
            </div>
          </div>
          <div class="education-item">
            <img src='images/snu_logo.png' alt="SNU Logo" class="institution-logo">
            <div class="education-details">
              <h3>B.S. in Mechanical & Aerospace Engineering</h3>
              <p><em>Mar 2015 - Feb 2021 | Seoul, Korea</em></p>
            </div>
          </div>
        </div>
      </section>

      <!-- Research Section -->
      <section class="section">
        <h2>Research</h2>
        <p>
          My research interest is understanding the interaction between agents and environments,
          and devising data-efficient decision-making (or robot learning) algorithms,
          especially in the field of reinforcement learning (RL).
        </p>
       
        
        <div class="research-item">
          <div class="research-image">
            <div class="one">
              <div class="two" id='d2c_image'>
                <img src='images/sequential_eqa.png' width="160" height="160">
              </div>
              <img src='images/sequential_eqa.png' width="160" height="160">
            </div>
          </div>
          <div class="research-content">
            <a href="https://arxiv.org/abs/2310.19261">
              <span class="papertitle">Sequential-EQA: A Memory-Centric Benchmark for Embodied VQA</span>
            </a>
            <br>
            Zikui Cai, Shivin Dass, <strong>Seungjae Lee</strong>, Mingyo Seo, Kaushal Janga, Aadi Palnitkar, Tan Dat Dao, Ruchit Rawal, Mintong Kang, Ruijie Zheng, Kaiyu Yue, Bo Li, Yuke Zhu, Roberto Mart√≠n-Mart√≠n, Tom Goldstein, Furong Huang
            <br>
            Under review
            <br>
            <!-- <p>
              In this work, we develop a method that uses out-of-distribution disagreement to diversify goal selection, enabling curriculum learning from only a few outcome examples.
            </p> -->
          </div>
        </div>

        <div class="research-item">
          <div class="research-image">
            <div class="one">
              <div class="two" id='d2c_image'>
                <img src='images/dasip.png' width="160" height="160">
              </div>
              <img src='images/dasip.png' width="160" height="160">
            </div>
          </div>
          <div class="research-content">
            <a href="https://arxiv.org/abs/2310.19261">
              <span class="papertitle">Dynamic Test-Time Compute Scaling in Control Policy: Difficulty-Aware Stochastic Interpolant Policy</span>
            </a>
            <br>
            Inkook Chun, <strong>Seungjae Lee</strong>, Michael Samuel Albergo, Saining Xie, Eric Vanden-Eijnden
            <br>
            <span class="tag red_transparent">NeurIPS, 2025</span> (Acceptance Rate: 24.52%)
            <br>
            <!-- <p>
              In this work, we develop a method that uses out-of-distribution disagreement to diversify goal selection, enabling curriculum learning from only a few outcome examples.
            </p> -->
          </div>
        </div>


        <div class="research-list">
          <div class="research-item">
            <div class="research-image">
              <div class="one">
                <div class="two" id='webai_video'>
                  <img src='images/ive_img.png' width="160" height="160">
                </div>
                <video width="160" height="160" muted autoplay loop style="object-fit: cover;">
                  <source src="images/ive_video.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
            <div class="research-content">
              <a href="https://ive-robot.github.io/">
                <span class="papertitle">Imagine, Verify, Execute: Memory-guided Agentic Exploration with Vision-Language Models</span>
              </a>
              <br>
              <strong>Seungjae Lee*</strong>, Daniel Ekpo*, Haowen Liu, Furong Huang‚Ä†, Abhinav Shrivastava‚Ä†, Jia-Bin Huang‚Ä†
              <br>
              <span class="tag red_transparent">CoRL, 2025</span> (Acceptance Rate: 35.77%)
              <br>
              (*equal contribution, ‚Ä†equal advising)
              <br>
              <a href="https://ive-robot.github.io/">project website</a> /
              <a href="https://arxiv.org/abs/2505.07815">arXiv</a>
              <p></p>
              <!-- <p>
                IVE (Imagine, Verify, Execute) is a vision-language model-driven framework that enables robots to imagine, verify, and execute physically plausible exploratory behaviors, leading to more diverse state coverage and exploration efficiency.
              </p> -->
            </div>
          </div>

        <div class="research-list">
          <div class="research-item">
            <div class="research-image">
              <div class="one">
                <div class="two" id='webai_video'>
                  <img src='images/whyweb_img.png' width="160" height="160">
                </div>
                <video width="160" height="160" muted autoplay loop style="object-fit: cover;">
                  <source src="images/iclrw_c.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
            <div class="research-content">
              <a href="https://vulnerable-ai-agents.github.io/">
                <span class="papertitle">Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis</span>
              </a>
              <br>
              Jeffrey Yang Fan Chiang*, <strong>Seungjae Lee*</strong>, Jia-Bin Huang, Furong Huang, Yizheng Chen
              <br>
              <span class="tag red_transparent">ICLRw, 2025</span>
              <br>
              (*equal contribution)
              <br>
              + ICLR 2025 Workshop Building Trust in Language Models and Applications
              <a href="https://vulnerable-ai-agents.github.io/">project website</a> /
              <a href="https://arxiv.org/abs/2502.20383">arXiv</a>
              <p></p>
              <!-- <p>
                Recent studies reveal alarming security flaws in Web AI agents‚Äîmaking them shockingly prone to adversarial attacks, even when built with safety-aligned LLMs. Our research uncovers why these vulnerabilities exist and how they compare to standalone LLMs.
              </p> -->
            </div>
          </div>

          <div class="research-item">
            <div class="research-image">
              <div class="one">
                <div class="two" id='rum_video'>
                  <img src='images/rum_button_thumbnail.png' width="160" height="160">
                </div>
                <video width="160" height="160" muted autoplay loop style="object-fit: cover;">
                  <source src="images/rum_button_video.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
            <div class="research-content">
              <a href="https://robotutilitymodels.com">
                <span class="papertitle">Robot Utility Models: General Policies for Zero-Shot Deployment in New Environments</span>
              </a>
              <br>
              Haritheja Etukuru, Norihito Naka, Zijin Hu, <strong>Seungjae Lee</strong>, Julian Mehu, Aaron Edsinger, Chris Paxton, Soumith Chintala, Lerrel Pinto, Nur Muhammad Mahi Shafiullah
              <br>
              <span class="tag red_transparent">ICRA, 2025</span>
              <br>
              + CoRL 2024 Workshop on Language and Robot Learning, "Oral"
              <br>
              <a href="https://robotutilitymodels.com/">project website</a>/
              <a href="https://robotutilitymodels.com/mfiles/paper/Robot_Utility_Models.pdf">paper</a>/
              <a href="https://github.com/haritheja-e/robot-utility-models/">github</a>
              <p></p>
              <!-- <p>
                Robot Utility Models (RUMs) is a simple method to build zero-shot robot policies that can solve useful tasks in completely new homes without any additional training often at 90%+ success rate.
              </p> -->
            </div>
          </div>

          <div class="research-item">
            <div class="research-image">
              <div class="one">
                <div class="two" id='vqbet_video'>
                  <img src='images/vqbet_button_thumbnail.png' width="160" height="160">
                </div>
                <video width="160" height="160" muted autoplay loop style="object-fit: cover;">
                  <source src="images/vqbet_button_video.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
            <div class="research-content">
              <a href="https://sjlee.cc/vq-bet">
                <span class="papertitle">Behavior Generation with Latent Actions</span>
              </a>
              <br>
              <strong>Seungjae Lee</strong>, Yibin Wang, Haritheja Etukuru, H. Jin Kim, Nur Muhammad Mahi Shafiullah, Lerrel Pinto
              <br>
              <span class="tag red_transparent">ICML, 2024 (Spotlight)</span> (Top: 3.5%)
              <br>
              + RSS 2024 Workshop SemRob, "Oral spotlights"
              <br>
              + ICML 2024 Workshop MFM-EAI, "Outstanding Paper Award - Winner"
              <br>
              <br>
              <a href="https://sjlee.cc/vq-bet">project website</a>/
              <a href="https://arxiv.org/abs/2403.03181">arXiv</a>/
              <a href="https://github.com/jayLEE0301/vq_bet_official">github</a>/
              <a href="https://github.com/huggingface/lerobot">ü§ó Lerobot Library</a>
              <p></p>
              <!-- <p>
                We present Vector-Quantized Behavior Transformer (VQ-BeT), a versatile model for multimodal action prediction, conditional generation, and partial observation handling.
              </p> -->
            </div>
          </div>

          <div class="research-item">
            <div class="research-image">
              <div class="one">
                <div class="two" id='cqm_image'>
                  <img src='images/cqm_image.png' width="160">
                </div>
                <video width="160" height="160" muted autoplay loop style="object-fit: cover;">
                  <source src="images/cqm_video.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
            <div class="research-content">
              <a href="https://arxiv.org/abs/2310.17330">
                <span class="papertitle">CQM: Curriculum Reinforcement Learning with a Quantized World Model</span>
              </a>
              <br>
              <strong>Seungjae Lee</strong>, Daesol Cho, Jonghae Park, H Jin Kim
              <br>
              <span class="tag red_transparent">NeurIPS, 2023</span> (Acceptance Rate: 26.07%)
              <br>
              <a href="https://arxiv.org/abs/2310.17330">arXiv</a>
              <p></p>
              <!-- <p>
                In this work, we presents a curriculum learning method that uses a quantized world model to automatically generate effective training goals in high-dimensional state spaces.
              </p> -->
            </div>
          </div>

          <div class="research-item">
            <div class="research-image">
              <div class="one">
                <div class="two" id='d2c_image'>
                  <img src='images/d2c_image2.png' width="160" height="160">
                </div>
                <img src='images/d2c_image.png' width="160" height="160">
              </div>
            </div>
            <div class="research-content">
              <a href="https://arxiv.org/abs/2310.19261">
                <span class="papertitle">Diversify & Conquer: Outcome-directed Curriculum RL via Out-of-Distribution Disagreement</span>
              </a>
              <br>
              Daesol Cho, <strong>Seungjae Lee</strong>, H Jin Kim
              <br>
              <span class="tag red_transparent">NeurIPS, 2023</span> (Acceptance Rate: 26.07%)
              <br>
              <a href="https://arxiv.org/abs/2310.19261">arXiv</a>
              <p></p>
              <!-- <p>
                In this work, we develop a method that uses out-of-distribution disagreement to diversify goal selection, enabling curriculum learning from only a few outcome examples.
              </p> -->
            </div>
          </div>

          <div class="research-item">
            <div class="research-image">
              <div class="one">
                <div class="two" id='snerl_image'>
                  <img src='images/snerl_image.png' width="160" height="160">
                </div>
                <video width="160" height="160" muted autoplay loop style="object-fit: cover;">
                  <source src="images/snerl_video.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
            <div class="research-content">
              <a href="https://arxiv.org/abs/2301.11520">
                <span class="papertitle">SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning</span>
              </a>
              <br>
              Dongseok Shim*, <strong>Seungjae Lee*</strong>, H Jin Kim
              <br>
              (*equal contribution)
              <br>
              <span class="tag red_transparent">ICML, 2023</span> (Acceptance Rate: 27.96%)
              <br>
              <a href="https://arxiv.org/abs/2301.11520">arXiv</a> /
              <a href="https://github.com/jayLEE0301/snerl_official">github</a>
              <p></p>
              <!-- <p>
                We present Semantic-aware Neural Radiance Fields for Reinforcement Learning (SNeRL), which jointly optimizes semantic-aware neural radiance fields (NeRF) with a convolutional encoder to learn 3D-aware neural implicit representation from multi-view images.
              </p> -->
            </div>
          </div>

          <div class="research-item">
            <div class="research-image">
              <div class="one">
                <div class="two" id='outpace_image'>
                  <img src='images/outpace_image.png' width="160" height="160">
                </div>
                <img src='images/outpace_image2.png' width="160" height="160">
              </div>
            </div>
            <div class="research-content">
              <a href="https://arxiv.org/abs/2301.11741">
                <span class="papertitle">Outcome-directed Reinforcement Learning by Uncertainty & Temporal Distance-Aware Curriculum Goal Generation</span>
              </a>
              <br>
              Daesol Cho*, <strong>Seungjae Lee*</strong>, H Jin Kim
              <br>
              (*equal contribution)
              <br>
              <span class="tag red_transparent">ICLR, 2023 (Spotlight)</span> (Top: 5.65%)
              <br>
              <a href="https://arxiv.org/abs/2301.11741">arXiv</a> /
              <a href="https://github.com/jayLEE0301/outpace_official">github</a>
              <p></p>
              <!-- <p>
                We propose an uncertainty & temporal distance-aware curriculum goal generation method for the outcome-directed RL via solving a bipartite matching problem. It can provide precisely calibrated guidance of the curriculum to the desired outcome states.
              </p> -->
            </div>
          </div>

          <div class="research-item">
            <div class="research-image">
              <div class="one">
                <div class="two" id='ijcas_image'>
                  <img src='images/ijcas_image2.png' width="160" height="160">
                </div>
                <img src='images/ijcas_image.png' width="160" height="100">
              </div>
            </div>
            <div class="research-content">
              <a href="https://jaylee0301.github.io">
                <span class="papertitle">Deep End-to-End Imitation Learning for Missile Guidance with Infrared Images</span>
              </a>
              <br>
              <strong>Seungjae Lee</strong>, Jongho Shin, Hyeong-Geun Kim, Daesol Cho, H. Jin Kim
              <br>
              <span class="tag red_transparent">IJCAS, 2023</span>
              <br>
              <p></p>
              <!-- <p>
                We propose an end-to-end missile guidance algorithm from raw infrared image pixels by imitating a conventional guidance law which leverages privileged data.
              </p> -->
            </div>
          </div>

          <div class="research-item">
            <div class="research-image">
              <div class="one">
                <div class="two" id='dhrl_image'>
                  <img src='images/dhrl_image.png' width="160" height="160">
                </div>
                <video width="160" height="160" muted autoplay loop style="object-fit: cover;">
                  <source src="images/dhrl_video.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
            <div class="research-content">
              <a href="https://arxiv.org/abs/2210.05150">
                <span class="papertitle">DHRL: A Graph-Based Approach for Long-Horizon and Sparse Hierarchical Reinforcement Learning</span>
              </a>
              <br>
              <strong>Seungjae Lee</strong>, Jigang Kim, Inkyu Jang, H. Jin Kim
              <br>
              <span class="tag red_transparent">NeurIPS, 2022 (Oral)</span> (Top: 1.76%)
              <br>
              <a href="https://arxiv.org/abs/2210.05150">arXiv</a> /
              <a href="https://github.com/jayLEE0301/dhrl_official">github</a>
              <p></p>
              <!-- <p>
                We present DHRL, a hierarchical reinforcement learning framework that uses a graph-based structure to improve exploration and long-term planning.
              </p> -->
            </div>
          </div>

          <div class="research-item">
            <div class="research-image">
              <div class="one">
                <div class="two" id='iros_image'>
                  <img src='images/iros_image2.png' width="160" height="160">
                </div>
                <video width="160" height="160" muted autoplay loop style="object-fit: cover;">
                  <source src="images/iros_c.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
            <div class="research-content">
              <a href="https://arxiv.org/abs/2107.06484">
                <span class="papertitle">Robust and Recursively Feasible Real-Time Trajectory Planning in Unknown Environments</span>
              </a>
              <br>
              Inkyu Jang, Dongjae Lee, <strong>Seungjae Lee</strong>, H Jin Kim
              <br>
              <span class="tag red_transparent">IROS, 2021</span>
              <br>
              <a href="https://arxiv.org/abs/2107.06484">arXiv</a>
              <p></p>
              <!-- <p>
                We proposes a trajectory planning algorithm that ensures robust, real-time navigation in unknown environments by maintaining recursive feasibility.
              </p> -->
            </div>
          </div>
        </div>
      </section>

      <!-- Projects Section -->
      <section class="section">
        <h2>Projects</h2>
        <div class="projects-list">
          <div class="project-item">
            <div class="project-image">
              <div class="one">
                <div class="two" id='hdx_image'>
                  <img src='images/hdx_image.png' width="160" height="160">
                </div>
                <video width="160" height="160" muted autoplay loop style="object-fit: cover;">
                  <source src="images/excavator.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
            <div class="project-content">
              <span class="papertitle">Training Excavator Virtual Driver based on Inverse RL</span>
              <br>
              <p>
                with HD Hyundai Heavy Industries Co., Ltd.
                <br>
                <em>Apr. 2023 - Mar. 2024</em>
              </p>
            </div>
          </div>

          <div class="project-item">
            <div class="project-image">
              <div class="one">
                <div class="two" id='ic01_image'>
                  <img src='images/ic01_image.png' width="160" height="160">
                </div>
                <video width="160" height="160" muted autoplay loop style="object-fit: cover;">
                  <source src="images/missile.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
            <div class="project-content">
              <span class="papertitle">End-to-End Machine Learning Based Guidance Research</span>
              <br>
              with Korean Agency for Defense Development (ADD)
              <br>
              <em>May. 2021 - Apr. 2023</em>
            </div>
          </div>
        </div>
      </section>

      <!-- Experience Section -->
      <section class="section">
        <h2>Experiences</h2>
        <div class="experience-list">
          <div class="experience-item">
            <img src='images/tri_logo.png' alt="TRI Logo" class="institution-logo">
            <div class="experience-details">
              <h3>Toyota Research Institute</h3>
              <p>Large Behavior Model Team Intern</p>
              <p><em>May 2025 - Aug 2025 | Boston, MA</em></p>
            </div>
          </div>

          <div class="experience-item">
            <img src='images/samsung_logo.png' alt="Samsung Logo" class="institution-logo">
            <div class="experience-details">
              <h3>Samsung Electronics</h3>
              <p>Deep Learning Algorithm Team Intern</p>
              <p><em>Jul 2020 - Sep 2020 | Gyunggi-do, Korea</em></p>
            </div>
          </div>

          <div class="experience-item">
            <img src='images/deepest.png' alt="Deepest Logo" class="institution-logo">
            <div class="experience-details">
              <h3>Deepest</h3>
              <p><em>Sep 2020 - Feb 2022 | Seoul, Korea</em></p>
            </div>
          </div>
        </div>
      </section>

      <!-- GitHub Projects Section -->
      <section class="section">
        <h2>GitHub Projects</h2>
        <div class="github-projects">
          <div class="pinned-repos">
            <div class="repo-row">
              <img src="https://github-readme-stats.vercel.app/api/pin/?username=jayLEE0301&repo=dhrl_official&theme=default&hide_border=true&bg_color=ffffff&title_color=333&text_color=666" alt="DHRL Repository" />
              <img src="https://github-readme-stats.vercel.app/api/pin/?username=jayLEE0301&repo=vq_bet_official&theme=default&hide_border=true&bg_color=ffffff&title_color=333&text_color=666" alt="VQ-BeT Repository" />
            </div>
            <div class="repo-row">
              <img src="https://github-readme-stats.vercel.app/api/pin/?username=jayLEE0301&repo=outpace_official&theme=default&hide_border=true&bg_color=ffffff&title_color=333&text_color=666" alt="OUTPACE Repository" />
              <img src="https://github-readme-stats.vercel.app/api/pin/?username=jayLEE0301&repo=snerl_official&theme=default&hide_border=true&bg_color=ffffff&title_color=333&text_color=666" alt="SNeRL Repository" />
            </div>
          </div>
          <div class="github-link">
            <a href="https://github.com/jayLEE0301" target="_blank">
              <span class="tag red">View All Projects on GitHub ‚Üí</span>
            </a>
          </div>
        </div>
      </section>

      <!-- Awards Section -->
      <section class="section">
        <h2>Awards and Achievements</h2>
        <ul class="awards-list">
          <li>[Scholarship] Dashin Songchon Foundation (Aug 2024 - Present)</li>
          <li>[Awards] Graduated Summa Cum Laude, Seoul National University (1st prize in Department of Aerospace Engineering)</li>
          <li>[Scholarship] Hyundai Motor Chung Mong-Koo Foundation (Aug 2021 - Jul2023)</li>
          <li>[Awards] NeurIPS Scholar Award</li>
          <li>[Awards] Global Excellence Scholarship 2022, Hyundai Motor Chung Mong-Koo Foundation</li>
          <li>[Awards] Best poster competition, SNU Artificial Intelligence Institute Spring Retreat</li>
          <li>[Awards] Global Excellence Scholarship 2023, Hyundai Motor Chung Mong-Koo Foundation</li>
        </ul>
      </section>

      <!-- Academic Services Section -->
      <section class="section">
        <h2>Academic Services</h2>
        <ul class="services-list">
          <li>Program Committee, RSS 2024 SemRob Workshop</li>
          <li>Conference reviewer for ICML'22 '24 '25</li>
          <li>Conference reviewer for IROS'23</li>
          <li>Conference reviewer for NeurIPS'23 '25</li>
          <li>Conference reviewer for ICLR'24 '25</li>
          <li>Conference reviewer for ICRA'24 '25</li>
          <li>Conference reviewer for AAAI'25</li>
          <li>Conference reviewer for CORL'25</li>
          <li>Conference reviewer for RSS'25</li>
        </ul>
      </section>

      <!-- Footer -->
      <footer class="footer">
        <p>Source code credit to <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a></p>
      </footer>
    </div>

    <!-- JavaScript for hover effects -->
    <script>
      // Common function for hover effects
      function setupHoverEffect(id) {
        const element = document.getElementById(id);
        if (element) {
          element.addEventListener('mouseover', () => element.style.opacity = "1");
          element.addEventListener('mouseout', () => element.style.opacity = "0");
        }
      }

      // Initialize all hover effects
      document.addEventListener('DOMContentLoaded', () => {
        const hoverElements = [
          'webai_video', 'rum_video', 'vqbet_video', 'cqm_image',
          'd2c_image', 'snerl_image', 'outpace_image', 'ijcas_image',
          'dhrl_image', 'iros_image', 'hdx_image', 'ic01_image'
        ];
        hoverElements.forEach(setupHoverEffect);
      });
    </script>
  </body>
</html>
